{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wu_observation_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TAT-gr9R_6hkNPHZ5tZ_WnRc8nQzi8bG",
      "authorship_tag": "ABX9TyM81MtV7jGfTHohX+8Ygt8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riverdogcabin/PSDS4900/blob/main/wu_observation_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsDzChS1eyOe"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DayLocator, DateFormatter\n",
        "from pprint import pprint\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoc5dRlgfB5s"
      },
      "source": [
        "with open('/content/drive/MyDrive/Capstone/PSDS4900/config.json') as configuration:\n",
        "  my_station = json.load(configuration).get('WU')[\"stationid\"] #'WU' is the parameters for WeatherUnderground"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpQfOgUefF-H",
        "outputId": "04d82144-5ac8-434a-872d-ce6871706228"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Capstone/PSDS4900/wu_data/wu_observations.csv')\n",
        "df['timestamp'] = pd.to_datetime(df['epoch'], unit='s')\n",
        "df = df.assign(timestamp_rounded=df.timestamp.dt.round('60min')) #rounded to the hour\n",
        "print('with dupes',df.shape)\n",
        "before = df.shape[0]\n",
        "df.drop_duplicates(inplace=True)\n",
        "print('without dupes',df.shape)\n",
        "print('difference: ',before-df.shape[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "with dupes (19504, 34)\n",
            "without dupes (19504, 34)\n",
            "difference:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIZ-8DsUqQPD"
      },
      "source": [
        "### Clean up the spikes (dupes in each timegroup as outline din the stats notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FWW7JGOfNMX"
      },
      "source": [
        "num_stations = len(df.stationID.unique())\n",
        "counted_df = df.groupby(['timestamp_rounded']).count().reset_index()\n",
        "hour_grouping_std = counted_df.stationID.std()\n",
        "problem_hours = list(counted_df[counted_df.stationID > num_stations+hour_grouping_std].timestamp_rounded) #get all groups that are more than one standard deviation off the station count\n",
        "indices_to_drop = df[(df.timestamp_rounded.isin(problem_hours)) & (df.timestamp > df.timestamp_rounded)].index\n",
        "df.drop(indices_to_drop,inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xejXVoCLiOD3"
      },
      "source": [
        "# uncomment the below to check to make sure the spikes are gone\n",
        "# grouped = df.groupby(['timestamp_rounded'])\n",
        "# ax = grouped.count().reset_index().windspeedAvg.plot(xlabel='Group Number',ylabel='Count',figsize=(20, 10))\n",
        "# ax.axhline(y=55,color='green',linestyle='--')\n",
        "# ax.axhline(y=num_stations,color='red',linestyle='--')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BPDHtYAqbpq"
      },
      "source": [
        "### Build correlation DataFrame and the dict of maximally correlated stations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUq4qbPVjD6o"
      },
      "source": [
        "columns_to_check = [s for s in df.columns if \"Avg\" in s]\n",
        "corr_df = pd.DataFrame({'stationID':df.stationID.unique()}) #create the shell of the dataframe to store the correlations\n",
        "max_correlations = {} #this will hold the maximally correlated stations for each variable\n",
        "for v in columns_to_check:\n",
        "  #create a pivot table for each variable\n",
        "  temp_df = df.pivot_table(index='timestamp_rounded',columns='stationID',values=v).corr()[[my_station]]\n",
        "  temp_df.columns.name = None #get rid of the column names and then collapse the indices, reindex and rename the columns\n",
        "  temp_df = temp_df.stack().reset_index().drop(columns='level_1').rename(columns={0:v+'_corr'})\n",
        "  #get rid of the results for my_station, obvs \n",
        "  temp_df = temp_df[temp_df.stationID != my_station]\n",
        "  #collect the maximally correlated station for the variable\n",
        "  max_correlations[v] = temp_df.loc[temp_df[v+'_corr'].abs().idxmax()]\n",
        "  #add all the corrleations for this variable as a column to the big correlation dataframe\n",
        "  corr_df = corr_df.merge(temp_df,on='stationID')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvGNYaPZrT6X"
      },
      "source": [
        "### Now let's build a simple linear regression model and test it. Found a[ great code snippet to set up k-fold cross validation](https://becominghuman.ai/linear-regression-in-python-with-pandas-scikit-learn-72574a2ec1a5) and adapted it. We'll start by just using the most highly-correlated station to create the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doextFAdjanK"
      },
      "source": [
        "target_stations = [my_station, max_correlations['windspeedAvg'].stationID]\n",
        "df_filtered = df[df.stationID.isin(target_stations)]\n",
        "df_reshaped = df_filtered.pivot(index='timestamp_rounded',columns='stationID',values='windspeedAvg')\n",
        "# df_reshaped"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fEET5ZEk7PL",
        "outputId": "7ff3429d-ab7c-4b22-d08f-50b9714b05b9"
      },
      "source": [
        "X = pd.DataFrame(df_reshaped.drop(columns=my_station))\n",
        "y = pd.DataFrame(df_reshaped[[my_station]]) #target\n",
        "\n",
        "model = LinearRegression()\n",
        "scores = []\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for i, (train, test) in enumerate(kfold.split(X, y)):\n",
        " model.fit(X.iloc[train,:], y.iloc[train,:])\n",
        " score = model.score(X.iloc[test,:], y.iloc[test,:])\n",
        " scores.append(score)\n",
        "print(scores)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.585806568136984, 0.6961795230891136, 0.7735785348303749]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tvg7tgZryHl"
      },
      "source": [
        "### Those scores aren't bad, but they aren't great. Let's see if adding more stations improves things. We'll take the top five most correlated stations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KOoGgkSmtHU",
        "outputId": "442e4668-7ce9-4fc1-8fcb-be90d84535fe"
      },
      "source": [
        "target_stations = corr_df.sort_values('windspeedAvg_corr').stationID.tail().to_list()\n",
        "target_stations.append(my_station)\n",
        "df_filtered = df[df.stationID.isin(target_stations)]\n",
        "df_reshaped = df_filtered.pivot_table(index='timestamp_rounded',columns='stationID',values='windspeedAvg')\n",
        "before = df_reshaped.shape[0]\n",
        "df_reshaped.dropna(inplace=True)\n",
        "print(\"dropping N/A rows reduced from {} to {}. Loss of {} rows\".format(before,df_reshaped.shape[0],before - df_reshaped.shape[0]))\n",
        "# pd.options.display.max_rows = 300 #so we can show the whole dataframe\n",
        "# df_reshaped"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dropping N/A rows reduced from 263 to 163. Loss of 100 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGgmokzVsNPM",
        "outputId": "6aca9a33-daad-469a-e92e-f7b41ec54e4f"
      },
      "source": [
        "X = pd.DataFrame(df_reshaped.drop(columns=my_station))\n",
        "y = pd.DataFrame(df_reshaped[[my_station]]) #target\n",
        "\n",
        "model = LinearRegression()\n",
        "scores = []\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for i, (train, test) in enumerate(kfold.split(X, y)):\n",
        " model.fit(X.iloc[train,:], y.iloc[train,:])\n",
        " score = model.score(X.iloc[test,:], y.iloc[test,:])\n",
        " scores.append(score)\n",
        "print(scores)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9010541746729965, 0.7270595151373567, 0.8617657323616549]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hJbMulx4Ev4"
      },
      "source": [
        "### Those got better by adding four more highly-correlated stations. Let's see what happens when we add all the stations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhyD89fdscGD",
        "outputId": "750d16d3-1dd6-450c-b941-cc8d1c2381c5"
      },
      "source": [
        "target_stations = corr_df.sort_values('windspeedAvg_corr').stationID.to_list()\n",
        "target_stations.append(my_station)\n",
        "df_filtered = df[df.stationID.isin(target_stations)]\n",
        "df_reshaped = df_filtered.pivot_table(index='timestamp_rounded',columns='stationID',values='windspeedAvg')\n",
        "before = df_reshaped.shape[0]\n",
        "df_reshaped.dropna(inplace=True)\n",
        "print(\"dropping N/A rows reduced from {} to {}. Loss of {} rows\".format(before,df_reshaped.shape[0],before - df_reshaped.shape[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dropping N/A rows reduced from 263 to 73. Loss of 190 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S7vPi3g6N6x",
        "outputId": "9a336900-ae95-4ef0-f333-5a2a2d9b14f1"
      },
      "source": [
        "X = pd.DataFrame(df_reshaped.drop(columns=my_station))\n",
        "y = pd.DataFrame(df_reshaped[[my_station]]) #target\n",
        "\n",
        "model = LinearRegression()\n",
        "scores = []\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for i, (train, test) in enumerate(kfold.split(X, y)):\n",
        " model.fit(X.iloc[train,:], y.iloc[train,:])\n",
        " score = model.score(X.iloc[test,:], y.iloc[test,:])\n",
        " scores.append(score)\n",
        "print(scores)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.46941616151915594, 0.27298305173122706, 0.7163418274249742]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq651py56U0w"
      },
      "source": [
        "### That did *not* improve things. I think that we had to drop too many rows because of missing values. Even though the hourly grouping was the _best_ way to align the data, it still means we lose lots of rows because of missing values. \n",
        "\n",
        "### The next few cells are a quick aside to figure out which stationIDs and and for what dates we are missing so much data. This will be used in another script to pull those stations/dates but the data exists here so it saves time to do it here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH6QzBzO6TX_"
      },
      "source": [
        "target_stations = corr_df.sort_values('windspeedAvg_corr').stationID.to_list()\n",
        "target_stations.append(my_station)\n",
        "df_filtered = df[df.stationID.isin(target_stations)]\n",
        "df_reshaped = df_filtered.pivot_table(index='timestamp_rounded',columns='stationID',values='windspeedAvg')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVGeaBMMH6A0",
        "outputId": "58ddbb7d-c923-4330-e91a-25350e8aafde"
      },
      "source": [
        "temp_df = df_reshaped.loc['2021-03-16 07:00:00'].T\n",
        "temp_df[temp_df.isna()].index.to_list()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['KCOCASTL167',\n",
              " 'KCOCASTL26',\n",
              " 'KCOCASTL308',\n",
              " 'KCOCASTL312',\n",
              " 'KCOCASTL321',\n",
              " 'KCOCASTL331',\n",
              " 'KCOCASTL342',\n",
              " 'KCOCASTL390',\n",
              " 'KCOCASTL397',\n",
              " 'KCOCASTL405',\n",
              " 'KCOCASTL430',\n",
              " 'KCOLITTL212',\n",
              " 'KCOLITTL697',\n",
              " 'KCOLITTL720',\n",
              " 'KCOLITTL771',\n",
              " 'KCOLITTL82',\n",
              " 'KCOLOUVI2',\n",
              " 'KCOSEDAL14',\n",
              " 'KCOSEDAL28',\n",
              " 'KCOSEDAL29',\n",
              " 'KCOSEDAL4',\n",
              " 'KCOSEDAL42',\n",
              " 'KCOSEDAL52',\n",
              " 'KCOSEDAL54',\n",
              " 'KCOSEDAL6',\n",
              " 'KCOSEDAL61',\n",
              " 'KCOSEDAL71',\n",
              " 'KCOSEDAL72',\n",
              " 'KCOSEDAL73',\n",
              " 'KCOSILVE32']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNaIa15FJrS-",
        "outputId": "9a126834-82f7-4c6d-af12-5d89aef848b3"
      },
      "source": [
        "temp_df = df_reshaped.T.loc['KCOCASTL167']\n",
        "# temp_df.head()\n",
        "date_set = set()\n",
        "for d in temp_df[temp_df.isna()].index.to_list():\n",
        "  date_set.add('{}{}{}'.format(d.year,d.month,d.day))\n",
        "\n",
        "date_set"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2021316', '2021317', '2021318', '2021319', '2021320'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkGQRdzwNuYE"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}